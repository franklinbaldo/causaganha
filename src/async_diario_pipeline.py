#!/usr/bin/env python3
"""
Async Diario Pipeline - Bulk download and archive TJRO diarios to Internet Archive

This script processes the pipeline-ready diarios list and handles:
- Concurrent PDF downloads from TJRO
- Parallel uploads to Internet Archive
- Progress tracking and resume capability
- Error handling and retry logic
- Rate limiting to be respectful to TJRO servers
"""

import argparse
import asyncio
import hashlib
import json
import logging
import os
import subprocess
import time
from concurrent.futures import ThreadPoolExecutor
from dataclasses import asdict, dataclass
from datetime import date, datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional

import aiohttp

from .anonymization_hooks import anonymize_metadata
from .pii_manager import PiiManager
from .config import load_config
from .database import DatabaseManager, CausaGanhaDB, run_db_migrations

# Environment variables are loaded from system environment


# Configure Internet Archive with environment variables
def configure_ia() -> bool:
    """Configure Internet Archive credentials from environment variables."""
    access_key = os.getenv("IA_ACCESS_KEY")
    secret_key = os.getenv("IA_SECRET_KEY")

    if not access_key or not secret_key:
        raise RuntimeError("IA_ACCESS_KEY or IA_SECRET_KEY not found in environment")

    try:
        import configparser
        from pathlib import Path

        # Create IA config file
        config_dir = Path.home() / ".config" / "internetarchive"
        config_dir.mkdir(parents=True, exist_ok=True)
        config_file = config_dir / "ia.ini"

        config = configparser.ConfigParser()
        config["s3"] = {"access": access_key, "secret": secret_key}
        config["general"] = {"screenname": "causaganha-bot"}
        config["cookies"] = {}

        with open(config_file, "w") as f:
            config.write(f)
        config_file.chmod(0o600)

        logging.info("Internet Archive configured successfully")
        return True
    except Exception as e:
        logging.warning(f"Failed to configure IA: {e}")
        return False


# Configuration (with environment variable support)
MAX_CONCURRENT_DOWNLOADS = int(
    os.getenv("MAX_CONCURRENT_DOWNLOADS", "3")
)  # Be respectful to TJRO servers
MAX_CONCURRENT_IA_UPLOADS = int(
    os.getenv("MAX_CONCURRENT_IA_UPLOADS", "2")
)  # Internet Archive rate limiting
DOWNLOAD_TIMEOUT = 300  # 5 minutes per PDF
RETRY_ATTEMPTS = 3
DELAY_BETWEEN_DOWNLOADS = 2.0  # Seconds between downloads
TRY_DIRECT_UPLOAD_DEFAULT = os.getenv("TRY_DIRECT_UPLOAD", "true").lower() == "true"


@dataclass
class ProcessingStatus:
    """Track processing status for each diario."""

    ia_identifier: str
    original_filename: str
    full_url: str
    date: str
    status: str = (
        "pending"  # pending, downloading, downloaded, uploading, completed, failed
    )
    local_path: Optional[str] = None
    ia_url: Optional[str] = None
    error_message: Optional[str] = None
    attempts: int = 0
    sha256_hash: Optional[str] = None
    file_size: Optional[int] = None
    processing_time: Optional[float] = None


class AsyncDiarioPipeline:
    """Main async pipeline for processing TJRO diarios."""

    def __init__(
        self,
        data_dir: Path = Path("data"),
        progress_file: Path = Path("data/diario_pipeline_progress.json"),
        max_concurrent_downloads: int = MAX_CONCURRENT_DOWNLOADS,
        max_concurrent_uploads: int = MAX_CONCURRENT_IA_UPLOADS,
        try_direct_upload: bool = True,
        anonymize_metadata: bool = False,
        pii_manager: PiiManager | None = None,
    ):
        self.data_dir = data_dir
        self.progress_file = progress_file
        self.max_concurrent_downloads = max_concurrent_downloads
        self.max_concurrent_uploads = max_concurrent_uploads
        self.try_direct_upload = try_direct_upload
        self.anonymize_metadata_flag = anonymize_metadata
        self.logger = logging.getLogger(__name__)

        if self.anonymize_metadata_flag:
            if pii_manager is None:
                config = load_config()
                db_path = Path(config["database"]["path"])
                db_manager = DatabaseManager(db_path=db_path)
                run_db_migrations(db_manager.db_path)
                db = CausaGanhaDB(db_manager=db_manager)
                self.pii_manager = PiiManager(db.conn)
            else:
                self.pii_manager = pii_manager
        else:
            self.pii_manager = None

        # Create directories
        self.data_dir.mkdir(exist_ok=True)
        (self.data_dir / "diarios").mkdir(exist_ok=True)

        # Processing tracking
        self.status_tracker: Dict[str, ProcessingStatus] = {}
        self.download_semaphore = asyncio.Semaphore(max_concurrent_downloads)
        self.upload_semaphore = asyncio.Semaphore(max_concurrent_uploads)

        # Session for HTTP requests
        self.session: Optional[aiohttp.ClientSession] = None

    async def __aenter__(self):
        """Async context manager entry."""
        timeout = aiohttp.ClientTimeout(total=DOWNLOAD_TIMEOUT)
        self.session = aiohttp.ClientSession(
            timeout=timeout,
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            },
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        if self.session:
            await self.session.close()

    def load_progress(self) -> None:
        """Load existing progress from file."""
        if self.progress_file.exists():
            try:
                with open(self.progress_file, "r") as f:
                    progress_data = json.load(f)

                self.status_tracker = {
                    key: ProcessingStatus(**data) for key, data in progress_data.items()
                }

                completed = len([
                    s for s in self.status_tracker.values() if s.status == "completed"
                ])
                total = len(self.status_tracker)
                self.logger.info(f"Loaded progress: {completed}/{total} completed")

            except Exception as e:
                self.logger.error(f"Failed to load progress: {e}")
                self.status_tracker = {}

    def save_progress(self) -> None:
        """Save current progress to file."""
        try:
            progress_data = {
                key: asdict(status) for key, status in self.status_tracker.items()
            }

            with open(self.progress_file, "w") as f:
                json.dump(progress_data, f, indent=2, default=str)

        except Exception as e:
            self.logger.error(f"Failed to save progress: {e}")

    def get_statistics(self) -> Dict:
        """Get current processing statistics."""
        statuses = [s.status for s in self.status_tracker.values()]

        return {
            "total": len(statuses),
            "pending": statuses.count("pending"),
            "downloading": statuses.count("downloading"),
            "downloaded": statuses.count("downloaded"),
            "uploading": statuses.count("uploading"),
            "completed": statuses.count("completed"),
            "failed": statuses.count("failed"),
            "completion_rate": statuses.count("completed") / len(statuses) * 100
            if statuses
            else 0,
        }

    async def download_pdf(self, diario_data: Dict, status: ProcessingStatus) -> bool:
        """Download a single PDF with retries."""
        async with self.download_semaphore:
            status.status = "downloading"
            status.attempts += 1
            start_time = time.time()

            try:
                local_path = (
                    self.data_dir / "diarios" / diario_data["original_filename"]
                )

                # Skip if already exists and has valid size
                if (
                    local_path.exists() and local_path.stat().st_size > 1000
                ):  # At least 1KB
                    status.local_path = str(local_path)
                    status.status = "downloaded"
                    status.file_size = local_path.stat().st_size
                    status.sha256_hash = await self._calculate_sha256(local_path)
                    self.logger.info(f"Skipping existing file: {local_path.name}")
                    return True

                self.logger.info(f"Downloading: {diario_data['full_url']}")

                async with self.session.get(diario_data["full_url"]) as response:
                    if response.status == 200:
                        content = await response.read()

                        # Validate it's actually a PDF
                        if not content.startswith(b"%PDF"):
                            raise ValueError("Downloaded content is not a valid PDF")

                        # Write to file
                        with open(local_path, "wb") as f:
                            f.write(content)

                        # Update status
                        status.local_path = str(local_path)
                        status.status = "downloaded"
                        status.file_size = len(content)
                        status.sha256_hash = hashlib.sha256(content).hexdigest()
                        status.processing_time = time.time() - start_time

                        self.logger.info(
                            f"Downloaded: {local_path.name} ({len(content):,} bytes)"
                        )

                        # Respectful delay
                        await asyncio.sleep(DELAY_BETWEEN_DOWNLOADS)
                        return True

                    else:
                        raise aiohttp.ClientResponseError(
                            request_info=response.request_info,
                            history=response.history,
                            status=response.status,
                            message=f"HTTP {response.status}",
                        )

            except Exception as e:
                status.error_message = str(e)
                self.logger.error(
                    f"Download failed for {diario_data['original_filename']}: {e}"
                )

                if status.attempts >= RETRY_ATTEMPTS:
                    status.status = "failed"
                    return False
                else:
                    status.status = "pending"  # Will retry
                    await asyncio.sleep(
                        min(status.attempts * 2, 10)
                    )  # Exponential backoff
                    return False

    async def _calculate_sha256(self, file_path: Path) -> str:
        """Calculate SHA256 hash of file asynchronously."""

        def _hash_file():
            sha256_hash = hashlib.sha256()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    sha256_hash.update(chunk)
            return sha256_hash.hexdigest()

        loop = asyncio.get_event_loop()
        with ThreadPoolExecutor() as executor:
            return await loop.run_in_executor(executor, _hash_file)

    async def check_ia_exists(self, ia_identifier: str) -> bool:
        """Check if item already exists in Internet Archive."""
        try:
            metadata_url = f"https://archive.org/metadata/{ia_identifier}"
            async with self.session.head(metadata_url) as response:
                return response.status == 200
        except Exception:
            return False

    def upload_to_ia_local(self, diario_data: Dict, status: ProcessingStatus) -> bool:
        """Upload PDF to Internet Archive from local file."""
        if not status.local_path or not Path(status.local_path).exists():
            self.logger.error(f"Local file not available for {status.ia_identifier}")
            return False

        status.status = "uploading"

        try:
            # Prepare IA metadata
            metadata = diario_data["metadata"].copy()
            metadata["sha256"] = status.sha256_hash
            metadata["originalurl"] = diario_data["full_url"]
            metadata["addeddate"] = datetime.now(timezone.utc).strftime(
                "%Y-%m-%d %H:%M:%S"
            )
            metadata["upload_method"] = "local_download_first"
            if self.pii_manager:
                metadata = anonymize_metadata(metadata, self.pii_manager)

            # Build ia command
            ia_cmd = [
                "ia",
                "upload",
                status.ia_identifier,
                status.local_path,
            ]

            # Add metadata as command line arguments
            for key, value in metadata.items():
                if value:  # Skip empty values
                    ia_cmd.extend([f"--metadata={key}:{value}"])

            # Execute upload
            self.logger.info(f"Uploading local file to IA: {status.ia_identifier}")
            result = subprocess.run(
                ia_cmd,
                capture_output=True,
                text=True,
                timeout=600,  # 10 minutes timeout
            )

            if result.returncode == 0:
                status.status = "completed"
                status.ia_url = f"https://archive.org/details/{status.ia_identifier}"
                self.logger.info(f"✅ Local IA upload completed: {status.ia_url}")

                # Remove local file to save space after successful upload
                try:
                    Path(status.local_path).unlink()
                    self.logger.info(f"Cleaned up local file: {status.local_path}")
                except Exception as cleanup_error:
                    self.logger.warning(
                        f"Failed to cleanup local file: {cleanup_error}"
                    )

                return True
            else:
                error_details = f"stdout: {result.stdout}, stderr: {result.stderr}"
                self.logger.error(
                    f"IA upload command failed with return code {result.returncode}: {error_details}"
                )
                raise subprocess.CalledProcessError(
                    result.returncode, ia_cmd, result.stdout, result.stderr
                )

        except Exception as e:
            status.error_message = str(e)
            status.status = "failed"
            self.logger.error(f"Local IA upload failed for {status.ia_identifier}: {e}")
            return False

    async def upload_to_ia_async(
        self, diario_data: Dict, status: ProcessingStatus
    ) -> bool:
        """Async wrapper for IA upload."""
        async with self.upload_semaphore:
            loop = asyncio.get_event_loop()
            with ThreadPoolExecutor() as executor:
                return await loop.run_in_executor(
                    executor, self.upload_to_ia_local, diario_data, status
                )

    async def process_diario(
        self, diario_data: Dict, skip_existing: bool = True
    ) -> bool:
        """Process a single diario: check if exists, download, then upload to IA."""
        ia_identifier = diario_data["ia_identifier"]

        # Get or create status tracker
        if ia_identifier not in self.status_tracker:
            self.status_tracker[ia_identifier] = ProcessingStatus(
                ia_identifier=ia_identifier,
                original_filename=diario_data["original_filename"],
                full_url=diario_data["full_url"],
                date=diario_data["date"],
            )

        status = self.status_tracker[ia_identifier]

        # Skip if already completed
        if status.status == "completed":
            self.logger.info(f"Already completed: {ia_identifier}")
            return True

        # Check if item already exists in IA
        if skip_existing and status.status == "pending":
            exists = await self.check_ia_exists(ia_identifier)
            if exists:
                status.status = "completed"
                status.ia_url = f"https://archive.org/details/{ia_identifier}"
                self.logger.info(f"✅ Item already exists in IA: {status.ia_url}")
                self.save_progress()
                return True

        # Download phase
        if status.status not in ["downloaded", "uploading"]:
            self.logger.info(f"Downloading {ia_identifier}")
            download_success = await self.download_pdf(diario_data, status)
            if not download_success:
                return False

            # Save progress after download
            self.save_progress()

        # Upload phase
        if status.status == "downloaded":
            upload_success = await self.upload_to_ia_async(diario_data, status)

            # Save progress after upload attempt
            self.save_progress()
            return upload_success

        return status.status == "completed"

    async def run_pipeline(
        self,
        diarios_data: List[Dict],
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        max_items: Optional[int] = None,
        skip_existing: bool = True,
    ) -> None:
        """Run the complete async pipeline."""

        # Filter by date range if specified
        if start_date or end_date:
            filtered_diarios = []
            for diario in diarios_data:
                diario_date = date.fromisoformat(diario["date"])

                include = True
                if start_date and diario_date < date.fromisoformat(start_date):
                    include = False
                if end_date and diario_date > date.fromisoformat(end_date):
                    include = False

                if include:
                    filtered_diarios.append(diario)

            diarios_data = filtered_diarios
            self.logger.info(f"Filtered to {len(diarios_data)} diarios in date range")

        # Limit number of items if specified
        if max_items:
            diarios_data = diarios_data[:max_items]
            self.logger.info(f"Limited to {max_items} diarios")

        self.logger.info(f"Starting pipeline for {len(diarios_data)} diarios")

        # Process diarios with controlled concurrency
        semaphore = asyncio.Semaphore(self.max_concurrent_downloads)

        async def process_with_semaphore(diario_data):
            async with semaphore:
                return await self.process_diario(
                    diario_data, skip_existing=skip_existing
                )

        # Create tasks
        tasks = [process_with_semaphore(diario) for diario in diarios_data]

        # Process with progress reporting
        completed = 0
        total = len(tasks)

        for i, task in enumerate(asyncio.as_completed(tasks)):
            try:
                await task
                completed += 1

                if completed % 10 == 0 or completed == total:
                    stats = self.get_statistics()
                    self.logger.info(
                        f"Progress: {completed}/{total} processed "
                        f"({stats['completion_rate']:.1f}% complete, "
                        f"{stats['failed']} failed)"
                    )

            except Exception as e:
                self.logger.error(f"Task failed: {e}")
                completed += 1

        # Final statistics
        final_stats = self.get_statistics()
        self.logger.info("Pipeline completed!")
        self.logger.info(f"Final statistics: {final_stats}")


async def main():
    """Main CLI interface."""
    parser = argparse.ArgumentParser(
        description="Async pipeline for TJRO diarios download and IA upload"
    )
    parser.add_argument(
        "--input",
        "-i",
        type=Path,
        default=Path("data/diarios_pipeline_ready.json"),
        help="Pipeline-ready JSON file",
    )
    parser.add_argument(
        "--data-dir",
        type=Path,
        default=Path("data"),
        help="Data directory for downloads",
    )
    parser.add_argument(
        "--max-items", type=int, help="Maximum number of items to process (for testing)"
    )
    parser.add_argument("--start-date", type=str, help="Start date filter (YYYY-MM-DD)")
    parser.add_argument("--end-date", type=str, help="End date filter (YYYY-MM-DD)")
    parser.add_argument(
        "--concurrent-downloads",
        type=int,
        default=MAX_CONCURRENT_DOWNLOADS,
        help="Max concurrent downloads",
    )
    parser.add_argument(
        "--concurrent-uploads",
        type=int,
        default=MAX_CONCURRENT_IA_UPLOADS,
        help="Max concurrent IA uploads",
    )
    parser.add_argument(
        "--resume", action="store_true", help="Resume from previous progress"
    )
    parser.add_argument(
        "--stats-only", action="store_true", help="Show statistics only"
    )
    parser.add_argument("--verbose", "-v", action="store_true", help="Verbose logging")
    parser.add_argument(
        "--force-reprocess",
        action="store_true",
        help="Reprocess items even if they already exist in IA",
    )
    parser.add_argument(
        "--anonymize-metadata",
        action="store_true",
        help="Replace creator and title metadata with UUIDs",
    )

    args = parser.parse_args()

    # Setup logging
    logging.basicConfig(
        level=logging.DEBUG if args.verbose else logging.INFO,
        format="%(asctime)s - %(levelname)s - %(message)s",
    )

    # Configure Internet Archive
    if not configure_ia():
        logging.error(
            "Failed to configure Internet Archive. Please check IA_ACCESS_KEY and IA_SECRET_KEY environment variables."
        )
        return 1

    # Load diarios data
    try:
        with open(args.input, "r") as f:
            diarios_data = json.load(f)
    except Exception as e:
        logging.error(f"Failed to load {args.input}: {e}")
        return 1

    # Initialize pipeline
    progress_file = args.data_dir / "diario_pipeline_progress.json"

    async with AsyncDiarioPipeline(
        data_dir=args.data_dir,
        progress_file=progress_file,
        max_concurrent_downloads=args.concurrent_downloads,
        max_concurrent_uploads=args.concurrent_uploads,
        anonymize_metadata=args.anonymize_metadata,
    ) as pipeline:
        # Load existing progress if resuming
        if args.resume:
            pipeline.load_progress()

        # Show stats if requested
        if args.stats_only:
            if pipeline.status_tracker:
                stats = pipeline.get_statistics()
                print("📊 Pipeline Statistics:")
                print(f"   Total: {stats['total']}")
                print(
                    f"   Completed: {stats['completed']} ({stats['completion_rate']:.1f}%)"
                )
                print(f"   Failed: {stats['failed']}")
                print(f"   Pending: {stats['pending']}")
                print(f"   In Progress: {stats['downloading'] + stats['uploading']}")
            else:
                print("No progress data found.")
            return 0

        # Run the pipeline
        await pipeline.run_pipeline(
            diarios_data,
            start_date=args.start_date,
            end_date=args.end_date,
            max_items=args.max_items,
            skip_existing=not args.force_reprocess,
        )


    return 0


if __name__ == "__main__":
    exit(asyncio.run(main()))
